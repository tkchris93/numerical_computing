\lab{Newton and Quasi-Newton Methods}{Newton and Quasi-Newton Methods}

\objective{Implement Newton's Method and several Quasi-Newton Methods and understand when and why to use each}

\section*{Overview of n-d Optimization}
We have already discussed using Newton's method to find the roots of an
equation. Newton's method is generally very useful because of its fast
convergence properties. However, Newton's method requires the explicit
calculation of the derivative (i.e., the Jacobian matrix) at each step,
which is computationally costly. In fact, while Newton's method is generally  $O(n^3)$,
the methods we will study reduce the complexity to $O(n^2)$. This class of methods is known as quasi-
Newton methods. In these methods, we will modify Newton's method so that the derivative (Jacobian)
does not have to be computed at each step, thus making computations faster.
This generally comes at the cost of slower convergence speed, but the increased
computation speed can make these methods more effective in many cases.

\section*{Newton's Method}
At this point, we have discussed Newton's Method several times. In the n dimensional version,
the next step is given by:

\begin{equation} \label{Eq:BasicNewton}
\mathbf{x}_{k+1} = \mathbf{x}_k - D^2f(\mathbf{x}_k)^{-1}Df(\mathbf{x}_k)^T
\end{equation}

\begin{problem}
Write code implementing Newton's method in n Dimensions. Ensure that it takes
the function, its Jacobian, and a Hessian matrix as arguments and returns the 
minimizer. Test it on the Rosenbrock function:
\[
f(x,y) = 100(y-x^2)^2 + (1-x)^2
\]
using starting points $(-2,2)$ and $(10,-10)$
\end{problem}

\section*{Broyden's Method}
One quasi-Newton method is known as Broyden's method. Broyden's Method is a
multidimensional version of the secant method we have discussed previously.
Just like the secant method approximates the second derivative of a function by
using the first derivatives at nearby points, Broyden's Method involves using
the Jacobian to update an initial Hessian matrix.

\subsection*{Broyden's Method in n Dimensions}

If we have the point $x_k$ and the Jacobian $J_k$ at that point we can use the
following equation to select our guess for the zero of the function:
\begin{equation} \label{Eq:BroydenSolve}
J_k \Delta x = -F(x_k)
\end{equation}

Where $\Delta x = x_{k+1}-x_k$. This is precisely Newton's method. However, since calculating the Jacobian at each step is costly, we instead use a generalization of secant lines to estimate the Jacobian (just as we used a secant line to approximate the derivative in the one-dimensional case).

The generalization for using secant lines is as follows: If we have points $x_k$ and $x_{k-1}$ then the Jacobian $J_k$ at the point $x_k$ will approximately satisfy the following equation:
\begin{equation}
J_k (x_k-x_{k-1}) \approx F(x_k) - F(x_{k-1})
\end{equation}
In the one dimensional problem we can use this equation to exactly specify the value $J_k$ (this is simply a finite difference approximation of the derivative). However, in multiple dimensions, the equation will be underdetermined (i.e. many $J_k$'s will satisfy the equation). However, suppose that we have a previous estimate of the Jacobian $J_{k-1}$ at the point $J_{k-1}$. We can then find the solution to the multi-dimensional secant equation above that minimizes $\|J_{k-1}-J_k\|$. This requirement is uniquely fulfilled by the following:
\begin{equation} \label{Eq:BroydenJacobian}
J_k = J_{k-1} + \frac{\Delta F-J_{k-1} \Delta x}{\|\Delta x\|^2}\Delta x^T
\end{equation}

In this equation the $\Delta F = F(x_k)-F(x_{k-1})$ and $\Delta x = x_k-x_{k-1}$. Intuitively, we are using the secant equation to make a rank-one update on the Jacobian at each step of the algorithm.

After we have obtained our secant line approximation of the Jacobian (Equation \ref{Eq:BroydenJacobian}) we can apply Equation \ref{Eq:BroydenSolve} to find $x_{k+1}$. We can then repeat this process until we have (presumably) converged to a zero of the function. 

\begin{problem}
Write code implementing Broyden's method. Test it on the function:
\[
f(x,y,z) = 
 \left( \begin{array}{ccc}
cos(xy)+ xz^3 \\
y^2 - y + x^2 \\
z + x-\frac{1}{2}-\sqrt[3]{2cos(\frac{1}{4})} \end{array} \right)
\]
using starting points $(1,2,3)$ and $(3,2,1)$
\end{problem}

\subsection*{BFGS}
We can often make Broyden's method faster using the Sherman-Morrison-Woodbury Formula.
When we make this adjustment, the method is now known as the Broyden-Fletcher-Goldfarb-Shanno, or BFGS, Method.
This formula allows us to efficiently calculate the inverse of a matrix when we add
a low rank update to that matrix. After manipulation of the Sherman-Morrison-Woodbury
Formula we obtain the following:

\[
J_k^{-1} = J_{k-1}^{-1} + \frac{\Delta x_k - J_{k-1}^{-1}\Delta F_k}{\Delta x_k^T J_{k-1}^{-1}\Delta F_k} (\Delta x_k^T J_{k-1}^{-1})
\]

Thus, we can calculate the inverse of the Jacobian in the first step of our algorithm
and then calculate an update to the inverse at each step using the above formula.

\begin{problem}
Implement this modification in a new function. Compare the two versions of Broyden's method on the function above.
Which is faster? Are there cases where one implementation is better than the other?
\end{problem}

\section*{Comparing Newton and Quasi-Newton Methods}

Different optimization algorithms are more efficient in different situations. If the
Jacobian and Hessian are readily available and the Hessian is easily inverted, the standard
Newton's Method is probably the best option. If the Hessian is not available or difficult to
compute, then using Broyden's Method may be a better option. In circumstances where computing
the inverse of the Hessian is difficult, BFGS will allow us to update the inverted Hessian
at each step without repeatedly inverting a matrix.

\begin{problem}
Compare the performance of Newton, Broyden, and BFGS on the following functions:
\begin{align*}
f(x,y) = 0.26(x^2+y^2) - 0.48xy
\end{align*}
\begin{align*}
f(x,y) = sin(x+y) + (x-y)^2 - 1.5x + 2.5y + 1
\end{align*}
Output the number of iterations of each algorithm as well as the total time
each algorithm takes to run. Calculate the time per iteration for each algorithm
and compare. You should see that Newton's Method takes more time per iteration, but
that is takes fewer steps than the other algorithms.
\end{problem}

\section*{The Gauss-Newton Method}
\subsection*{Non-linear Least Squares Problems}
We now discuss a very important class of problems known as Least Squares problems. These
are unconstrained optimization problems that seek to minimize an objective function of the form
$$
f(x) = \frac{1}{2}\displaystyle\sum_{j=1}^m r_j^2(x),
$$
where each $r_i : \mathbb{R}^n \rightarrow \mathbb{R}$ is smooth, and $m \geq n$. Such problems
arise in many scientific fields, including economics, physics, and statistics. Linear Least
Squares problems form an important subclass, and can be solved directly without the need for an
iterative method. At present we will focus on the non-linear case, which can be solved with a
line search method.

To motivate the problem further, suppose you are given a set of data points, and you have some kind of model for the data.
You need to choose particular values for the parameters in your model, and you wish to do so in a way that ``best fits"
the observed data. What do we mean by ``best fit"? We need some way to measure the error between our model and the data set,
and then minimize this error. The best fit will correspond to the choice of parameters that minimize the error function.

More formally, suppose we are given the data points $(t_1, y_1), (t_2, y_2), \ldots, (t_m, y_m)$, where $y_i \in \mathbb{R}$
and $t_i \in \mathbb{R}^n$ for $i = 1, \ldots, m$. Let $\phi(x, t)$ be our model for this data set, where $x$ is
a vector of parameters of the model, and $t \in \mathbb{R}^n$. We can measure the error at the $i$-th data point by the value
$$r_i(x) := \phi(x, t_i) - y_i,$$ and by summing the squares of these errors, we obtain our non-linear least squares objective
function:
$$
f(x) = \frac{1}{2} \displaystyle \sum_{j=1}^m  r_j^2(x).
$$

The individual functions $r_i$ that measure the error between the model and the data point are known as \emph{residuals},
and we can aggregate these functions into a \emph{residual vector}
$$
r(x) := (r_1(x), r_2(x), \ldots, r_m(x))^T.
$$
The Jacobian of $r(x)$ can be expressed in terms of the gradients of each $r_i$ as follows:
$$
J(x) = \begin{bmatrix} \nabla r_1(x)^T \\ \nabla r_2(x)^T \\ \vdots \\ \nabla r_m(x)^T \end{bmatrix}
$$
You can further verify that
\begin{align*}
\nabla f(x) &= J(x)^T r(x), \\
\nabla^2 f(x) &= J(x)^TJ(x) + \displaystyle \sum_{j=1}^m r_j(x) \nabla^2r_j(x).
\end{align*}
That second term in the formula for $\nabla^2 f$ involves second derivatives and can be problematic to compute. Often in practice,
this term is small, either because the residuals themselves are small, or are nearly affine in a neighborhood of the solution and
hence the second derivatives are small.
The simplest method for solving the nonlinear least squares problem, known as the \emph{Gauss-Newton Method}, exploits this
observation, simply ignoring the second term and making the approximation
$$
\nabla^2 f(x) \approx J(x)^TJ(x).
$$
The method then proceeds in a manner similar to Newton's Method. In particular, at the $k$-th iteration, we choose a search
direction $p_k$ that solves the linear system
$$
J_k^TJ_kp_k = -J_k^Tr_k.
$$

For convenience, we summarize these steps in Algorithm \ref{alg:guassnewton}.

\begin{algorithm}
\begin{algorithmic}[1]
\Procedure{Gauss-Newton}{}
    \State \textrm{Choose initial parameter vector } $x_0$
    \State $k \gets 0$
    \While{$J_k^Tr_k \neq 0$}
        \State \textrm{solve } $J_k^TJ_kp_k = -J_k^Tr_k$
        \State \textrm{choose step size } $\alpha_k$ \textrm{ satisfying Wolfe Conditions.}
        \State $x_{k+1} \gets x_k + \alpha_kp_k$
        \State $k \gets k+1$
    \EndWhile
\EndProcedure
\end{algorithmic}
\caption{Gauss-Newton Method}
\label{alg:guassnewton}
\end{algorithm}

\begin{problem}
Implement the Gauss-Newton Method. Your function should take a function,
its derivative, the residual vector $r$, and the Jacobian of the residual
vector $J$. Return the minimizer.

Feel free to use SciPy functions to solve linear systems and calculate step sizes
in your algorithm. You might find optimize.line\_search especially useful.
\end{problem}

Let us work through an example of a nonlinear least squares problem. Suppose we have data points
generated from a sine function and slightly perturbed by gaussian noise. In Python we can generate such
data as follows:
\begin{lstlisting}
>>> t = np.arange(10)
>>> y = 3*np.sin(0.5*t)+ 0.5*np.random.randn(10)
\end{lstlisting}
Now we write Python functions for our model, the residual vector, the Jacobian, the objective function,
and the gradient. The calculations for all of these are straight forward.
\begin{lstlisting}
>>> def model(x, t):
>>>     return x[0]*np.sin(x[1]*t)
>>> def residual(x):
>>>     return model(x, t) - y
>>> def jac(x):
>>>     ans = np.empty((10,2))
>>>     ans[:,0] = np.sin(x[1]*t)
>>>     ans[:,1] = x[0]*t*np.cos(x[1]*t)
>>>     return ans
>>> def objective(x):
>>>     return .5*(residual(x)**2).sum()
>>> def grad(x):
>>>     return jac(x).T.dot(residual(x))
\end{lstlisting}
By inspecting our data, we might make an initial guess for the parameters $x_0 = (2.5, 0.6)$.
We are now ready to use our \li{gaussNewton} function to find the least squares solution.
\begin{lstlisting}
>>> x0 = np.array([2.5,.6])
>>> x = gaussNewton(objective, grad, jac, residual, x0, niter=10)
\end{lstlisting}
We can plot everything together to compare our fitted model with the data and the original sine
curve from which the data were generated.
\begin{lstlisting}
dom = np.linspace(0,10,100)
plt.plot(t, y, '*')
plt.plot(dom, 3*np.sin(.5*dom), '--')
plt.plot(dom, x[0]*np.sin(x[1]*dom))
plt.show()
\end{lstlisting}

\subsection*{Non-linear Least Squares in Python}
The module \li{scipy.optimize} also has a method to solve non-linear least squares problem, and it
is quite convenient. The function is called \li{leastsq}, and in its most basic use, you only need
to pass in the residual function and starting point as arguments. In the example above, we simply
need to execute the following code:
\begin{lstlisting}
>>> from scipy.optimize import leastsq
>>> x2 = leastsq(residual, x0)[0]
\end{lstlisting}
This should give us the same answer, but much faster.
\begin{problem}
We have census data giving the population of the United States every ten years since 1790.
For convenience, we have entered the data in Python below, so that you may simply copy and
paste.
\begin{lstlisting}
>>> #Start with the first 8 decades of data
>>> years1 = np.arange(8)
>>> pop1 = np.array([3.929, 5.308, 7.240, 9.638, 12.866,
>>>                 17.069, 23.192, 31.443])
>>>
>>> #Now consider the first 16 decades
>>> years2 = np.arange(16)
>>> pop2 = np.array([3.929, 5.308, 7.240, 9.638, 12.866,
>>>                 17.069, 23.192, 31.443, 38.558, 50.156,
>>>                 62.948, 75.996, 91.972, 105.711, 122.775,
>>>                 131.669])
\end{lstlisting}
Consider just the first 8 decades of population data. By plotting the data and having
an inclination that population growth tends to be exponential, it is reasonable to
hypothesize an exponential model for the population, that is,
$$
\phi(x_1,x_2,x_3,t) = x_1\exp(x_2(t+x_3)).
$$
By inspection, find a reasonable
initial guess for the parameters $(x_1, x_2, x_3)$ (i.e. $(150, .4, 2.5)$).
Write a function for this model in Python, along with the corresponding residual vector,
and fit the model using the \li{leastsq} function.  Plot the data against the fitted curve,
to see how close you are.

Now consider all 16 decades of data. If you plot your curve from above with this more complete
data, you will see that the model is no longer a good fit. Instead, the data suggest
a logistic model, which also arises from a differential equations treatment of population growth.
Thus, your new model is
$$
\phi(x_1,x_2,x_3,t) = \frac{x_1}{1+\exp(-x_2(t+x_3))}.
$$
By inspection, find a reasonable
initial guess for the parameters $(x_1, x_2, x_3)$ (i.e. $(150, .4, -15)$).
Again, write Python functions for the model and the corresponding residual vector,
and fit the model. Plot the data against the fitted curve. It should be a good fit.
\end{problem} 
